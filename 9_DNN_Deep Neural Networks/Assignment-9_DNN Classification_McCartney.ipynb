{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Deep Neural Network - Classification - McCartney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will focus transportation mode detection. It contains data about smarthphone sensors as people are using different modes of transportation. Each row represents a specific mode of transportation and the corresponding readings in the smartphone sensors (for that mode of transportation). This is a multiclass classification task: predict whether the mode of transportation based on a smartphone's sensor readings. Transportation mode prediction can provide context information to enhance applications and provide a better user experience. It can be crucial for many different applications, such as device profiling, monitoring road and traffic conditions, healthcare, travel support etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Variables\n",
    "\n",
    "The description of variables are provided in \"Transportation - Data Dictionary.docx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Use the **transportation.csv** data set and build a model to predict **target**. Build at least **three neural network models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:\n",
    "\n",
    "Please save and submit this Jupyter notebook file. The correctness of the code matters for your grade. **Readability and organization of your code is also important.** You may lose points for submitting unreadable/undecipherable code. Therefore, use markdown cells to create sections, and use comments where necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Prepare the Data\n",
    "## Also, perform feature engineering: create one new variable from existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>androidsensoraccelerometermean</th>\n",
       "      <th>androidsensoraccelerometermin</th>\n",
       "      <th>androidsensoraccelerometermax</th>\n",
       "      <th>androidsensoraccelerometerstd</th>\n",
       "      <th>androidsensorgyroscopemean</th>\n",
       "      <th>androidsensorgyroscopemin</th>\n",
       "      <th>androidsensorgyroscopemax</th>\n",
       "      <th>androidsensorgyroscopestd</th>\n",
       "      <th>soundmean</th>\n",
       "      <th>soundmin</th>\n",
       "      <th>soundmax</th>\n",
       "      <th>soundstd</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.701276</td>\n",
       "      <td>9.520186</td>\n",
       "      <td>9.826383</td>\n",
       "      <td>0.074273</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.033699</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>88.111708</td>\n",
       "      <td>88.111708</td>\n",
       "      <td>88.111708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.240731</td>\n",
       "      <td>6.927889</td>\n",
       "      <td>11.371441</td>\n",
       "      <td>1.157460</td>\n",
       "      <td>0.114340</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.291155</td>\n",
       "      <td>0.071189</td>\n",
       "      <td>89.771860</td>\n",
       "      <td>89.771860</td>\n",
       "      <td>89.771860</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.397750</td>\n",
       "      <td>8.649993</td>\n",
       "      <td>11.105722</td>\n",
       "      <td>0.614978</td>\n",
       "      <td>0.301824</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.339794</td>\n",
       "      <td>0.021420</td>\n",
       "      <td>89.797764</td>\n",
       "      <td>89.797764</td>\n",
       "      <td>89.797764</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.197564</td>\n",
       "      <td>9.977808</td>\n",
       "      <td>10.417198</td>\n",
       "      <td>0.137675</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.039349</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>89.746162</td>\n",
       "      <td>89.746162</td>\n",
       "      <td>89.746162</td>\n",
       "      <td>0.122969</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.812239</td>\n",
       "      <td>8.290205</td>\n",
       "      <td>11.913782</td>\n",
       "      <td>1.016052</td>\n",
       "      <td>0.109497</td>\n",
       "      <td>0.005390</td>\n",
       "      <td>0.277880</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>89.768758</td>\n",
       "      <td>89.768758</td>\n",
       "      <td>89.768758</td>\n",
       "      <td>1.023191</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   androidsensoraccelerometermean  androidsensoraccelerometermin  \\\n",
       "0                        9.701276                       9.520186   \n",
       "1                        9.240731                       6.927889   \n",
       "2                       10.397750                       8.649993   \n",
       "3                       10.197564                       9.977808   \n",
       "4                        9.812239                       8.290205   \n",
       "\n",
       "   androidsensoraccelerometermax  androidsensoraccelerometerstd  \\\n",
       "0                       9.826383                       0.074273   \n",
       "1                      11.371441                       1.157460   \n",
       "2                      11.105722                       0.614978   \n",
       "3                      10.417198                       0.137675   \n",
       "4                      11.913782                       1.016052   \n",
       "\n",
       "   androidsensorgyroscopemean  androidsensorgyroscopemin  \\\n",
       "0                    0.021729                   0.012970   \n",
       "1                    0.114340                   0.018950   \n",
       "2                    0.301824                   0.260226   \n",
       "3                    0.017742                   0.003502   \n",
       "4                    0.109497                   0.005390   \n",
       "\n",
       "   androidsensorgyroscopemax  androidsensorgyroscopestd  soundmean   soundmin  \\\n",
       "0                   0.033699                   0.005727  88.111708  88.111708   \n",
       "1                   0.291155                   0.071189  89.771860  89.771860   \n",
       "2                   0.339794                   0.021420  89.797764  89.797764   \n",
       "3                   0.039349                   0.009800  89.746162  89.746162   \n",
       "4                   0.277880                   0.070489  89.768758  89.768758   \n",
       "\n",
       "    soundmax  soundstd target  \n",
       "0  88.111708  0.000000  Train  \n",
       "1  89.771860  0.008778    Bus  \n",
       "2  89.797764  0.039875    Car  \n",
       "3  89.746162  0.122969  Train  \n",
       "4  89.768758  1.023191    Bus  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transportation = pd.read_csv(\"transportation.csv\")\n",
    "transportation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(transportation, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "androidsensoraccelerometermean    0\n",
       "androidsensoraccelerometermin     0\n",
       "androidsensoraccelerometermax     0\n",
       "androidsensoraccelerometerstd     0\n",
       "androidsensorgyroscopemean        0\n",
       "androidsensorgyroscopemin         0\n",
       "androidsensorgyroscopemax         0\n",
       "androidsensorgyroscopestd         0\n",
       "soundmean                         0\n",
       "soundmin                          0\n",
       "soundmax                          0\n",
       "soundstd                          0\n",
       "target                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "androidsensoraccelerometermean    0\n",
       "androidsensoraccelerometermin     0\n",
       "androidsensoraccelerometermax     0\n",
       "androidsensoraccelerometerstd     0\n",
       "androidsensorgyroscopemean        0\n",
       "androidsensorgyroscopemin         0\n",
       "androidsensorgyroscopemax         0\n",
       "androidsensorgyroscopestd         0\n",
       "soundmean                         0\n",
       "soundmin                          0\n",
       "soundmax                          0\n",
       "soundstd                          0\n",
       "target                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use the following columns in this tutorial, because they are not for binary classification tasks\n",
    "\n",
    "train = train_set.drop([], axis=1)\n",
    "test = test_set.drop([], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train[['target']]\n",
    "test_target = test[['target']]\n",
    "\n",
    "train_inputs = train.drop(['target'], axis=1)\n",
    "test_inputs = test.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    429\n",
       "0.003665     19\n",
       "0.001728     15\n",
       "0.002732      7\n",
       "0.061094      6\n",
       "           ... \n",
       "0.067099      1\n",
       "0.534092      1\n",
       "1.195875      1\n",
       "0.001704      1\n",
       "2.176901      1\n",
       "Name: androidsensorgyroscopemean, Length: 2995, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['androidsensorgyroscopemean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARyElEQVR4nO3df6zd9V3H8edLQCTrfrAwr7WtFpNq5EfEcYOYRXObLVI3DZi4pAsZTGeqhCUzIVHYH05jGvlDZgQdWoUMMlzTuM2SCRokuy4mMFYIWkrFVanY0dBMJuPOBS2+/eN8a46X097Tc27PPaef5yM5Od/z+X4/3+/7+4G8+j2f8z3npqqQJLXhO9a6AEnS5Bj6ktQQQ1+SGmLoS1JDDH1Jasi5a13ASi666KLavHnzSH2/9a1v8aY3vWl1C5qAWa0brH0tzGrdMLu1z0LdTz755Ner6h3L26c+9Ddv3sy+fftG6ru4uMjCwsLqFjQBs1o3WPtamNW6YXZrn4W6k/zroHandySpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSFT/43ccez/2it86Na/nPhxD9/+vokfU5KG4ZW+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGrBj6STYl+WKSg0kOJPlo1/6bSb6W5Onu8d6+PrclOZTkuSTX9LVfmWR/t+7OJDkzpyVJGmSYv5F7HLilqp5K8mbgySSPdOt+r6p+t3/jJJcA24FLge8F/ibJD1bV68DdwA7gceAhYBvw8OqciiRpJSte6VfV0ap6qlt+FTgIbDhFl2uB3VX1WlU9DxwCrkqyHnhLVT1WVQXcD1w37glIkoZ3WnP6STYDPwp8uWv6SJJ/SHJvkgu7tg3Av/V1O9K1beiWl7dLkiYkvYvuITZM1gF/C+ysqs8lmQO+DhTw28D6qvrFJH8IPFZVn+763UNvKucF4Heq6j1d+08Av1ZVPzvgWDvoTQMxNzd35e7du0c6uWMvv8JL3x6p61gu3/DWsfovLS2xbt26Vapmsqx98ma1bpjd2meh7q1btz5ZVfPL24eZ0yfJecBngQeq6nMAVfVS3/o/Ab7QvTwCbOrrvhF4sWvfOKD9DapqF7ALYH5+vhYWFoYp8w3uemAvd+wf6hRX1eHrF8bqv7i4yKjnvNasffJmtW6Y3dpntW4Y7u6dAPcAB6vqE33t6/s2+zngmW75QWB7kvOTXAxsAZ6oqqPAq0mu7vZ5A7B3lc5DkjSEYS6D3wV8ENif5Omu7WPAB5JcQW965zDwywBVdSDJHuBZenf+3NzduQNwE/Ap4AJ6d+14544kTdCKoV9VfwcMup/+oVP02QnsHNC+D7jsdAqUJK0ev5ErSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JekhqwY+kk2JflikoNJDiT5aNf+9iSPJPlq93xhX5/bkhxK8lySa/rar0yyv1t3Z5KcmdOSJA0yzJX+ceCWqvph4Grg5iSXALcCj1bVFuDR7jXduu3ApcA24JNJzun2dTewA9jSPbat4rlIklawYuhX1dGqeqpbfhU4CGwArgXu6za7D7iuW74W2F1Vr1XV88Ah4Kok64G3VNVjVVXA/X19JEkTkF7+Drlxshn4EnAZ8EJVva1v3Teq6sIkfwA8XlWf7trvAR4GDgO3V9V7uvafAH69qn5mwHF20HtHwNzc3JW7d+8e6eSOvfwKL317pK5juXzDW8fqv7S0xLp161apmsmy9smb1bphdmufhbq3bt36ZFXNL28/d9gdJFkHfBb41ar65imm4wetqFO0v7GxahewC2B+fr4WFhaGLfP/ueuBvdyxf+hTXDWHr18Yq//i4iKjnvNas/bJm9W6YXZrn9W6Yci7d5KcRy/wH6iqz3XNL3VTNnTPx7r2I8Cmvu4bgRe79o0D2iVJEzLM3TsB7gEOVtUn+lY9CNzYLd8I7O1r357k/CQX0/vA9omqOgq8muTqbp839PWRJE3AMHMf7wI+COxP8nTX9jHgdmBPkg8DLwDvB6iqA0n2AM/Su/Pn5qp6vet3E/Ap4AJ68/wPr85pSJKGsWLoV9XfMXg+HuDdJ+mzE9g5oH0fvQ+BJUlrwG/kSlJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSErhn6Se5McS/JMX9tvJvlakqe7x3v71t2W5FCS55Jc09d+ZZL93bo7k2T1T0eSdCrDXOl/Ctg2oP33quqK7vEQQJJLgO3ApV2fTyY5p9v+bmAHsKV7DNqnJOkMWjH0q+pLwMtD7u9aYHdVvVZVzwOHgKuSrAfeUlWPVVUB9wPXjVizJGlE547R9yNJbgD2AbdU1TeADcDjfdsc6dr+u1te3j5Qkh303hUwNzfH4uLiSAXOXQC3XH58pL7jGLXeE5aWlsbex1qx9smb1bphdmuf1bph9NC/G/htoLrnO4BfBAbN09cp2geqql3ALoD5+flaWFgYqci7HtjLHfvH+XdtNIevXxir/+LiIqOe81qz9smb1bphdmuf1bphxLt3quqlqnq9qv4H+BPgqm7VEWBT36YbgRe79o0D2iVJEzRS6Hdz9Cf8HHDizp4Hge1Jzk9yMb0PbJ+oqqPAq0mu7u7auQHYO0bdkqQRrDj3keQzwAJwUZIjwMeBhSRX0JuiOQz8MkBVHUiyB3gWOA7cXFWvd7u6id6dQBcAD3cPSdIErRj6VfWBAc33nGL7ncDOAe37gMtOqzpJ0qryG7mS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyIqhn+TeJMeSPNPX9vYkjyT5avd8Yd+625IcSvJckmv62q9Msr9bd2eSrP7pSJJOZZgr/U8B25a13Qo8WlVbgEe71yS5BNgOXNr1+WSSc7o+dwM7gC3dY/k+JUln2IqhX1VfAl5e1nwtcF+3fB9wXV/77qp6raqeBw4BVyVZD7ylqh6rqgLu7+sjSZqQc0fsN1dVRwGq6miS7+7aNwCP9213pGv77255eftASXbQe1fA3Nwci4uLoxV5Adxy+fGR+o5j1HpPWFpaGnsfa8XaJ29W64bZrX1W64bRQ/9kBs3T1ynaB6qqXcAugPn5+VpYWBipmLse2Msd+1f7FFd2+PqFsfovLi4y6jmvNWufvFmtG2a39lmtG0a/e+elbsqG7vlY134E2NS33Ubgxa5944B2SdIEjRr6DwI3dss3Anv72rcnOT/JxfQ+sH2imwp6NcnV3V07N/T1kSRNyIpzH0k+AywAFyU5AnwcuB3Yk+TDwAvA+wGq6kCSPcCzwHHg5qp6vdvVTfTuBLoAeLh7SJImaMXQr6oPnGTVu0+y/U5g54D2fcBlp1WdJGlV+Y1cSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWSs0E9yOMn+JE8n2de1vT3JI0m+2j1f2Lf9bUkOJXkuyTXjFi9JOj2rcaW/taquqKr57vWtwKNVtQV4tHtNkkuA7cClwDbgk0nOWYXjS5KGdCamd64F7uuW7wOu62vfXVWvVdXzwCHgqjNwfEnSSaSqRu+cPA98Ayjgj6tqV5L/qKq39W3zjaq6MMkfAI9X1ae79nuAh6vqzwfsdwewA2Bubu7K3bt3j1TfsZdf4aVvj9R1LJdveOtY/ZeWlli3bt0qVTNZ1j55s1o3zG7ts1D31q1bn+ybgfk/546533dV1YtJvht4JMk/nmLbDGgb+C9OVe0CdgHMz8/XwsLCSMXd9cBe7tg/7imevsPXL4zVf3FxkVHPea1Z++TNat0wu7XPat0w5vROVb3YPR8DPk9vuualJOsBuudj3eZHgE193TcCL45zfEnS6Rk59JO8KcmbTywDPwU8AzwI3NhtdiOwt1t+ENie5PwkFwNbgCdGPb4k6fSNM/cxB3w+yYn9/FlV/VWSrwB7knwYeAF4P0BVHUiyB3gWOA7cXFWvj1W9JOm0jBz6VfUvwI8MaP934N0n6bMT2DnqMSVJ4/EbuZLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQcf5cok5i861/OVb/Wy4/zodG3Mfh29831rElnd280pekhhj6ktQQp3c088adTjvhdKfVnErTLPJKX5IaYuhLUkOc3tGqODHFMs6dR5LOPEP/LLNa89uSzk5O70hSQ7zSl0a0lu+qvHNIo/JKX5IaMvEr/STbgN8HzgH+tKpun3QN0qzrf5cxyx+eD1u772xWz0Sv9JOcA/wh8NPAJcAHklwyyRokqWWTvtK/CjhUVf8CkGQ3cC3w7ITrkDRDpu2utEm8uzpT725SVWdkxwMPlvw8sK2qfql7/UHgx6rqI8u22wHs6F7+EPDciIe8CPj6iH3X0qzWDda+Fma1bpjd2meh7u+vqncsb5z0lX4GtL3hX52q2gXsGvtgyb6qmh93P5M2q3WDta+FWa0bZrf2Wa0bJn/3zhFgU9/rjcCLE65Bkpo16dD/CrAlycVJvhPYDjw44RokqVkTnd6pquNJPgL8Nb1bNu+tqgNn8JBjTxGtkVmtG6x9Lcxq3TC7tc9q3ZP9IFeStLb8Rq4kNcTQl6SGnBWhn2RbkueSHEpy64D1SXJnt/4fkrxzLepcboi6F5K8kuTp7vEba1HncknuTXIsyTMnWT+V4w1D1T6tY74pyReTHExyIMlHB2wzdeM+ZN3TOubfleSJJH/f1f5bA7aZujFfUVXN9IPeB8L/DPwA8J3A3wOXLNvmvcDD9L4ncDXw5RmpewH4wlrXOqD2nwTeCTxzkvVTN96nUfu0jvl64J3d8puBf5qR/8+HqXtaxzzAum75PODLwNXTPuYrPc6GK/3/+2mHqvov4MRPO/S7Fri/eh4H3pZk/aQLXWaYuqdSVX0JePkUm0zjeAND1T6VqupoVT3VLb8KHAQ2LNts6sZ9yLqnUjeOS93L87rH8jtfpm7MV3I2hP4G4N/6Xh/hjf9TDbPNpA1b0493by8fTnLpZEob2zSO9+mY6jFPshn4UXpXnv2metxPUTdM6ZgnOSfJ08Ax4JGqmqkxH+Rs+CMqw/y0w1A//zBhw9T0FL3fz1hK8l7gL4AtZ7qwVTCN4z2sqR7zJOuAzwK/WlXfXL56QJepGPcV6p7aMa+q14ErkrwN+HySy6qq//OgqR3zkzkbrvSH+WmHafz5hxVrqqpvnnh7WVUPAecluWhyJY5sGsd7KNM85knOoxecD1TV5wZsMpXjvlLd0zzmJ1TVfwCLwLZlq6ZyzE/lbAj9YX7a4UHghu6T9quBV6rq6KQLXWbFupN8T5J0y1fR++/17xOv9PRN43gPZVrHvKvpHuBgVX3iJJtN3bgPU/cUj/k7uit8klwAvAf4x2WbTd2Yr2Tmp3fqJD/tkORXuvV/BDxE71P2Q8B/Ar+wVvWeMGTdPw/clOQ48G1ge3W3DKylJJ+hd8fFRUmOAB+n9yHX1I73CUPUPpVjDrwL+CCwv5tjBvgY8H0w1eM+TN3TOubrgfvS++NP3wHsqaovTHu2rMSfYZCkhpwN0zuSpCEZ+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakh/wtOf/wnlBVtfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs['androidsensorgyroscopemean'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import power transformer from sklearn. It will help us create a \"normal distribution\"\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "PT = PowerTransformer(method = 'yeo-johnson', standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_androidsensorgyroscopemean = PT.fit_transform(train_inputs[['androidsensorgyroscopemean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'0'}>]], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(transformed_androidsensorgyroscopemean).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "androidsensoraccelerometermean    float64\n",
       "androidsensoraccelerometermin     float64\n",
       "androidsensoraccelerometermax     float64\n",
       "androidsensoraccelerometerstd     float64\n",
       "androidsensorgyroscopemean        float64\n",
       "androidsensorgyroscopemin         float64\n",
       "androidsensorgyroscopemax         float64\n",
       "androidsensorgyroscopestd         float64\n",
       "soundmean                         float64\n",
       "soundmin                          float64\n",
       "soundmax                          float64\n",
       "soundstd                          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['androidsensoraccelerometermean',\n",
       " 'androidsensoraccelerometermin',\n",
       " 'androidsensoraccelerometermax',\n",
       " 'androidsensoraccelerometerstd',\n",
       " 'androidsensorgyroscopemean',\n",
       " 'androidsensorgyroscopemin',\n",
       " 'androidsensorgyroscopemax',\n",
       " 'androidsensorgyroscopestd',\n",
       " 'soundmean',\n",
       " 'soundmin',\n",
       " 'soundmax',\n",
       " 'soundstd']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_columns = ['androidsensorgyroscopemean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_column = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('powertransformer', PowerTransformer(method = 'yeo-johnson', standardize=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        #('cat', categorical_transformer, categorical_columns),\n",
    "        #('binary', binary_transformer, binary_columns),\n",
    "        ('trans', my_new_column, transformed_columns)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "#passtrough is an optional step. You don't have to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96502736,  0.18813864, -0.47059151, ..., -2.03871059,\n",
       "        -0.46595122, -0.54204567],\n",
       "       [-0.44993714,  0.39374271, -0.45801539, ...,  0.41327939,\n",
       "         0.45606711, -0.49684724],\n",
       "       [-0.2398556 , -0.1440191 , -0.0460587 , ...,  0.11094777,\n",
       "        -0.06080159, -0.91181332],\n",
       "       ...,\n",
       "       [ 0.81775282,  0.66660056, -0.16738671, ...,  0.2894401 ,\n",
       "        -0.32030219,  0.14341749],\n",
       "       [ 0.91256496,  0.92118101, -0.15963158, ...,  0.12363989,\n",
       "        -0.17193402, -0.07642557],\n",
       "       [-0.00741434,  0.86171927, -0.4353211 , ...,  0.7542238 ,\n",
       "        -0.46525574, -0.83020435]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47186822,  0.67141103, -0.46687855, ..., -0.50464325,\n",
       "        -0.11498009, -0.91181332],\n",
       "       [-0.19484422,  0.32231464, -0.22875245, ..., -2.03871059,\n",
       "        -0.46595122, -0.64136491],\n",
       "       [-0.35212761,  0.68145389, -0.45852856, ...,  0.15745292,\n",
       "         0.80725399, -0.82512189],\n",
       "       ...,\n",
       "       [ 0.53222872,  0.38577108, -0.15379124, ...,  0.26856399,\n",
       "        -0.46595122,  1.24627251],\n",
       "       [ 2.7725744 , -1.4377839 ,  1.19230982, ..., -2.03871059,\n",
       "        -0.46595122,  1.92998202],\n",
       "       [ 0.35875065,  0.77523588, -0.24882557, ...,  0.6100174 ,\n",
       "        -0.46595122,  1.36387137]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>Still</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "3298    Bus\n",
       "1662  Train\n",
       "1528  Train\n",
       "3651  Train\n",
       "4988    Car\n",
       "...     ...\n",
       "1504    Bus\n",
       "2662  Train\n",
       "4234  Train\n",
       "882   Train\n",
       "3113  Still\n",
       "\n",
       "[3500 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras needs Ordinal target values for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       ...,\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "train_y = ord_enc.fit_transform(train_target)\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [4.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = ord_enc.transform(test_target)\n",
    "\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Bus', 'Car', 'Still', 'Train', 'Walking'], dtype=object)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_enc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target \n",
       "Still      0.203714\n",
       "Walking    0.203429\n",
       "Car        0.202000\n",
       "Train      0.196000\n",
       "Bus        0.194857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.value_counts()/len(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras DNN model 1 - Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=13)) #match nerons with column count or train shape (train_x.shape[1])\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 22,004\n",
      "Trainable params: 22,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 4 which is outside the valid range of [0, 4).  Label values: 4 2 4 4 4 3 0 4 3 1 1 3 0 1 4 1 4 3 3 2 4 2 1 4 0 0 1 0 3 0 4 1 4 1 3 2 4 0 3 0 1 4 0 2 1 2 3 0 4 1 3 1 1 0 1 3 1 4 2 0 0 3 1 2 3 2 1 1 0 0 0 2 1 0 2 3 0 1 2 0 0 4 2 0 2 1 3 4 1 2 2 2 2 4 3 4 4 1 3 1 3 3 2 0 3 3 0 1 4 0 0 4 1 2 4 1 3 3 0 2 1 3 0 3 0 4 2 1 3 2 2 2 1 2 4 0 1 1 1 2 0 3 3 4 2 4 3 2 4 1 2 4 1 2 0 3 4 2 4 3 2 1 0 2 1 0 2 3 0 4 2 2 3 4 4 2 0 2 0 2 0 4 4 1 1 0 0 1 1 1 2 0 2 3 4 0 4 2 1 1 0 4 1 1 4 2 1 4 1 4 3 0 4 3 4 2 2 4 4 2 0 1 3 0 1 0 3 3 2 4 2 2 2 2 1 1 0 4 2 0 0 1 3 0 1 4 2 1 0 3 1 2 2 2 0 1 0 0 2 1 2 2 2 2 1 0 2 0 0 1 2 2 3 0 0 2 2 4 3 0 0 4 2 1 0 0 1 3 2 1 1 3 3 1 4 0 2 0 2 1 3 1 2 2 0 4 4 2 0 4 4 0 3 4 3 3 1 1 0 2 0 4 1 1 2 0 0 4 1 1 2 0 3 0 1 4 1 2 4 4 3 2 1 1 0 4 3 0 0 2 2 2 3 1 1 0 2 1 3 3 2 3 2 3 3 4 1 2 3 4 0 4 1 3 1 1 2 4 3 1 0 2 1 4 0 1 4 2 0 1 2 2 0 3 3 4 4 0 4 2 0 3 1 2 2 3 0 4 2 3 1 4 2 2 2 3 0 2 1 1 4 3 0 3 0 4 3 3 2 2 4 3 4 3 4 3 3 2 0 1 4 4 4 0 1 3 4 0 1 1 4 1 2 3 1 2 3 3 0 0 3 3 2 4 1 0 0 1 0 1 4 0 4 0 3 2 2 4 3 2 2 4 4 4 3 0 2 2 0 0 3 4 0 3 1 3 4 0 4 3\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-56-4f941ad93d35>:3) ]] [Op:__inference_train_function_681]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-4f941ad93d35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model.fit(train_x, train_y, \n\u001b[0m\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     epochs=20, batch_size=500)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Received a label value of 4 which is outside the valid range of [0, 4).  Label values: 4 2 4 4 4 3 0 4 3 1 1 3 0 1 4 1 4 3 3 2 4 2 1 4 0 0 1 0 3 0 4 1 4 1 3 2 4 0 3 0 1 4 0 2 1 2 3 0 4 1 3 1 1 0 1 3 1 4 2 0 0 3 1 2 3 2 1 1 0 0 0 2 1 0 2 3 0 1 2 0 0 4 2 0 2 1 3 4 1 2 2 2 2 4 3 4 4 1 3 1 3 3 2 0 3 3 0 1 4 0 0 4 1 2 4 1 3 3 0 2 1 3 0 3 0 4 2 1 3 2 2 2 1 2 4 0 1 1 1 2 0 3 3 4 2 4 3 2 4 1 2 4 1 2 0 3 4 2 4 3 2 1 0 2 1 0 2 3 0 4 2 2 3 4 4 2 0 2 0 2 0 4 4 1 1 0 0 1 1 1 2 0 2 3 4 0 4 2 1 1 0 4 1 1 4 2 1 4 1 4 3 0 4 3 4 2 2 4 4 2 0 1 3 0 1 0 3 3 2 4 2 2 2 2 1 1 0 4 2 0 0 1 3 0 1 4 2 1 0 3 1 2 2 2 0 1 0 0 2 1 2 2 2 2 1 0 2 0 0 1 2 2 3 0 0 2 2 4 3 0 0 4 2 1 0 0 1 3 2 1 1 3 3 1 4 0 2 0 2 1 3 1 2 2 0 4 4 2 0 4 4 0 3 4 3 3 1 1 0 2 0 4 1 1 2 0 0 4 1 1 2 0 3 0 1 4 1 2 4 4 3 2 1 1 0 4 3 0 0 2 2 2 3 1 1 0 2 1 3 3 2 3 2 3 3 4 1 2 3 4 0 4 1 3 1 1 2 4 3 1 0 2 1 4 0 1 4 2 0 1 2 2 0 3 3 4 4 0 4 2 0 3 1 2 2 3 0 4 2 3 1 4 2 2 2 3 0 2 1 1 4 3 0 3 0 4 3 3 2 2 4 3 4 3 4 3 3 2 0 1 4 4 4 0 1 3 4 0 1 1 4 1 2 3 1 2 3 3 0 0 3 3 2 4 1 0 0 1 0 1 4 0 4 0 3 2 2 4 3 2 2 4 4 4 3 0 2 2 0 0 3 4 0 3 1 3 4 0 4 3\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-56-4f941ad93d35>:3) ]] [Op:__inference_train_function_681]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras DNN model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to try deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier (one model for comparison purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "Briefly answer the following questions: (2 points) \n",
    "1) Which model performs the best (and why)?<br>\n",
    "2) What is the baseline value? <br>\n",
    "3) Does the best model perform better than the baseline (and why)?<br>\n",
    "4) Does the best model exhibit any overfitting; what did you do about it?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
