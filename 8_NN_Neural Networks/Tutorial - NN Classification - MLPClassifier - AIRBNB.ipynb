{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Neural Network - Classification\n",
    "\n",
    "\n",
    "We will predict the price category, among 4 categories, of an AIRBNB listing (`price_category` column). This is a multi-class classification task.\n",
    "\n",
    "**The unit of analysis is an AIRBNB LISTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>price_per_extra_person</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>number_days_btw_first_last_review</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>price</th>\n",
       "      <th>price_gte_150</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.282619</td>\n",
       "      <td>-71.133068</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moderate</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>gte_226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.286241</td>\n",
       "      <td>-71.134374</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>804</td>\n",
       "      <td>94.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.292438</td>\n",
       "      <td>-71.135765</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>2574</td>\n",
       "      <td>98.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.281106</td>\n",
       "      <td>-71.121021</td>\n",
       "      <td>House</td>\n",
       "      <td>Private room</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.284512</td>\n",
       "      <td>-71.136258</td>\n",
       "      <td>House</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>380</td>\n",
       "      <td>99.0</td>\n",
       "      <td>flexible</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>btw_75-150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_is_superhost  host_identity_verified neighbourhood_cleansed  \\\n",
       "0                  0                       0             Roslindale   \n",
       "1                  0                       1             Roslindale   \n",
       "2                  1                       1             Roslindale   \n",
       "3                  0                       0             Roslindale   \n",
       "4                  1                       1             Roslindale   \n",
       "\n",
       "    latitude  longitude property_type        room_type  accommodates  \\\n",
       "0  42.282619 -71.133068         House  Entire home/apt             4   \n",
       "1  42.286241 -71.134374     Apartment     Private room             2   \n",
       "2  42.292438 -71.135765     Apartment     Private room             2   \n",
       "3  42.281106 -71.121021         House     Private room             4   \n",
       "4  42.284512 -71.136258         House     Private room             2   \n",
       "\n",
       "   bathrooms  bedrooms  ...  guests_included price_per_extra_person  \\\n",
       "0        1.5       2.0  ...                1                      0   \n",
       "1        1.0       1.0  ...                0                      0   \n",
       "2        1.0       1.0  ...                1                     20   \n",
       "3        1.0       1.0  ...                2                     25   \n",
       "4        1.5       1.0  ...                1                      0   \n",
       "\n",
       "   minimum_nights  number_of_reviews  number_days_btw_first_last_review  \\\n",
       "0               2                  0                                  0   \n",
       "1               2                 36                                804   \n",
       "2               3                 41                               2574   \n",
       "3               1                  1                                  0   \n",
       "4               2                 29                                380   \n",
       "\n",
       "   review_scores_rating  cancellation_policy  price  price_gte_150  \\\n",
       "0                   NaN             moderate    250              1   \n",
       "1                  94.0             moderate     65              0   \n",
       "2                  98.0             moderate     65              0   \n",
       "3                 100.0             moderate     75              0   \n",
       "4                  99.0             flexible     79              0   \n",
       "\n",
       "  price_category  \n",
       "0        gte_226  \n",
       "1         lte_75  \n",
       "2         lte_75  \n",
       "3         lte_75  \n",
       "4     btw_75-150  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will predict the \"price_gte_150\" value in the data set:\n",
    "\n",
    "airbnb = pd.read_csv(\"airbnb.csv\")\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(airbnb, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful: we haven't seperated the target column yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                       0\n",
       "host_identity_verified                  0\n",
       "neighbourhood_cleansed                  0\n",
       "latitude                                0\n",
       "longitude                               0\n",
       "property_type                           8\n",
       "room_type                               0\n",
       "accommodates                            0\n",
       "bathrooms                              19\n",
       "bedrooms                               19\n",
       "beds                                   16\n",
       "bed_type                                0\n",
       "Number of amenities                     0\n",
       "guests_included                         0\n",
       "price_per_extra_person                  0\n",
       "minimum_nights                          0\n",
       "number_of_reviews                       0\n",
       "number_days_btw_first_last_review       0\n",
       "review_scores_rating                 1609\n",
       "cancellation_policy                     0\n",
       "price                                   0\n",
       "price_gte_150                           0\n",
       "price_category                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                      0\n",
       "host_identity_verified                 0\n",
       "neighbourhood_cleansed                 0\n",
       "latitude                               0\n",
       "longitude                              0\n",
       "property_type                          1\n",
       "room_type                              0\n",
       "accommodates                           0\n",
       "bathrooms                             17\n",
       "bedrooms                              11\n",
       "beds                                   8\n",
       "bed_type                               0\n",
       "Number of amenities                    0\n",
       "guests_included                        0\n",
       "price_per_extra_person                 0\n",
       "minimum_nights                         0\n",
       "number_of_reviews                      0\n",
       "number_days_btw_first_last_review      0\n",
       "review_scores_rating                 674\n",
       "cancellation_policy                    0\n",
       "price                                  0\n",
       "price_gte_150                          0\n",
       "price_category                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the variables we can't use in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use the following columns in this tutorial, because they are not for binary classification tasks\n",
    "\n",
    "train = train_set.drop(['price', 'price_gte_150'], axis=1)\n",
    "test = test_set.drop(['price', 'price_gte_150'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the target variable (we don't want to transform it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['price_category']\n",
    "test_y = test['price_category']\n",
    "\n",
    "train_inputs = train.drop(['price_category'], axis=1)\n",
    "test_inputs = test.drop(['price_category'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Let's derive a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7190.000000\n",
       "mean        3.031154\n",
       "std         6.592199\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max       273.000000\n",
       "Name: minimum_nights, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's analyze \"minimum_nights\"\n",
    "\n",
    "train_inputs['minimum_nights'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      2901\n",
       "2      1980\n",
       "3      1220\n",
       "7       254\n",
       "4       239\n",
       "5       185\n",
       "10      144\n",
       "30       57\n",
       "14       50\n",
       "15       43\n",
       "6        35\n",
       "20       15\n",
       "28       13\n",
       "32        9\n",
       "25        8\n",
       "60        6\n",
       "90        5\n",
       "27        4\n",
       "9         4\n",
       "17        3\n",
       "13        3\n",
       "23        3\n",
       "8         2\n",
       "21        2\n",
       "11        2\n",
       "273       2\n",
       "18        1\n",
       "Name: minimum_nights, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['minimum_nights'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs['minimum_nights'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new categorical column: convert the minimum nights into 4 categories, equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3437    (0.999, 2.0]\n",
       "6622    (3.0, 273.0]\n",
       "2262    (0.999, 2.0]\n",
       "2246      (2.0, 3.0]\n",
       "835       (2.0, 3.0]\n",
       "            ...     \n",
       "5734      (2.0, 3.0]\n",
       "5191    (0.999, 2.0]\n",
       "5390    (3.0, 273.0]\n",
       "860     (0.999, 2.0]\n",
       "7270    (0.999, 2.0]\n",
       "Name: minimum_nights, Length: 7190, dtype: category\n",
       "Categories (3, interval[float64]): [(0.999, 2.0] < (2.0, 3.0] < (3.0, 273.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qcut creates n quantiles (it is a discretization technique)\n",
    "# Let's create 5 quantiles:\n",
    "pd.qcut(train_inputs['minimum_nights'], 5, duplicates='drop')   \n",
    "\n",
    "# Notice, it can't do 5 quantiles because of the overlapping edges. It can only 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       4881\n",
       "medium    1220\n",
       "high      1089\n",
       "Name: minimum_nights, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(train_inputs['minimum_nights'],5, duplicates='drop',\n",
    "        labels=['low', 'medium', 'high']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_col(df):\n",
    "    #Create a copy so that we don't overwrite the existing dataframe\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    df1['quantile_min_nights'] = pd.qcut(train_inputs['minimum_nights'],5, duplicates='drop',\n",
    "                        labels=['low', 'medium', 'high'])\n",
    "    \n",
    "\n",
    "    return df1[['quantile_min_nights']]\n",
    "    # You can use this to check whether the calculation is made correctly:\n",
    "    #return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Identify the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                      int64\n",
       "host_identity_verified                 int64\n",
       "neighbourhood_cleansed                object\n",
       "latitude                             float64\n",
       "longitude                            float64\n",
       "property_type                         object\n",
       "room_type                             object\n",
       "accommodates                           int64\n",
       "bathrooms                            float64\n",
       "bedrooms                             float64\n",
       "beds                                 float64\n",
       "bed_type                              object\n",
       "Number of amenities                    int64\n",
       "guests_included                        int64\n",
       "price_per_extra_person                 int64\n",
       "minimum_nights                         int64\n",
       "number_of_reviews                      int64\n",
       "number_days_btw_first_last_review      int64\n",
       "review_scores_rating                 float64\n",
       "cancellation_policy                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this stage, you can manually identify numeric, binary, and categorical columns as follows:**\n",
    "\n",
    "`numeric_columns = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'Number of amenities', 'guests_included', 'price_per_extra_person', 'minimum_nights', 'number_of_reviews', 'number_days_btw_first_last_review', 'review_scores_rating']`\n",
    " \n",
    " `binary_columns = ['host_is_superhost', 'host_identity_verified']`\n",
    " \n",
    " `categorical_columns = ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'cancellation_policy']`\n",
    " \n",
    "<br>\n",
    " \n",
    "**If you do not want to manually type these, you can do the below tricks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the binary columns so we can pass them through without transforming\n",
    "binary_columns = ['host_is_superhost', 'host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: numerical columns already includes the binary columns,\n",
    "# So, we need to remove the binary columns from numerical columns.\n",
    "\n",
    "for col in binary_columns:\n",
    "    numeric_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost', 'host_identity_verified']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'Number of amenities',\n",
       " 'guests_included',\n",
       " 'price_per_extra_person',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'number_days_btw_first_last_review',\n",
       " 'review_scores_rating']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbourhood_cleansed',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'bed_type',\n",
       " 'cancellation_policy']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_columns = ['minimum_nights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_column = Pipeline(steps=[('my_new_column', FunctionTransformer(new_col)),\n",
    "                               ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                               ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('binary', binary_transformer, binary_columns),\n",
    "        ('trans', my_new_column, feat_eng_columns)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "#passtrough is an optional step. You don't have to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: fit_transform() for TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.10940159, -1.39824237,  1.20477863, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.61906783, -1.38593382, -1.16133947, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.14448465, -0.16705969, -1.16133947, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.82039585,  0.74441303, -0.56980994, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.92762441,  0.3821493 , -0.56980994, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.34071414, -0.53929512, -1.16133947, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7190, 64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform: transform() for TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63069768,  0.40533687,  1.79630816, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.15153485,  0.27611111, -0.56980994, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-2.02789334, -0.91924215,  0.02171958, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.15906806, -0.38872897,  1.20477863, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.11838687, -0.56878308, -0.56980994, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.94171792,  0.19283558, -1.16133947, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3082, 64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "btw_75-150     0.332823\n",
       "btw_151-225    0.241725\n",
       "lte_75         0.214743\n",
       "gte_226        0.210709\n",
       "Name: price_category, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts()/len(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erich\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Default settings create 1 hidden layer with 100 neurons\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948817802503477"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.773523685918235"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[437,  75, 174,  18],\n",
       "       [111, 836,  48, 104],\n",
       "       [ 72,  21, 579,   3],\n",
       "       [  8,  64,   0, 532]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase maximum iterations for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23339131\n",
      "Iteration 2, loss = 0.99515419\n",
      "Iteration 3, loss = 0.89690122\n",
      "Iteration 4, loss = 0.84936494\n",
      "Iteration 5, loss = 0.82098676\n",
      "Iteration 6, loss = 0.79945932\n",
      "Iteration 7, loss = 0.78277379\n",
      "Iteration 8, loss = 0.76842074\n",
      "Iteration 9, loss = 0.75633568\n",
      "Iteration 10, loss = 0.74517936\n",
      "Iteration 11, loss = 0.73627119\n",
      "Iteration 12, loss = 0.72612645\n",
      "Iteration 13, loss = 0.71777432\n",
      "Iteration 14, loss = 0.70986074\n",
      "Iteration 15, loss = 0.70108521\n",
      "Iteration 16, loss = 0.69422622\n",
      "Iteration 17, loss = 0.68804445\n",
      "Iteration 18, loss = 0.67815615\n",
      "Iteration 19, loss = 0.67058389\n",
      "Iteration 20, loss = 0.66315490\n",
      "Iteration 21, loss = 0.65702492\n",
      "Iteration 22, loss = 0.64966308\n",
      "Iteration 23, loss = 0.64298975\n",
      "Iteration 24, loss = 0.63635492\n",
      "Iteration 25, loss = 0.62807489\n",
      "Iteration 26, loss = 0.62166893\n",
      "Iteration 27, loss = 0.61543551\n",
      "Iteration 28, loss = 0.60970299\n",
      "Iteration 29, loss = 0.60197841\n",
      "Iteration 30, loss = 0.59545301\n",
      "Iteration 31, loss = 0.59080918\n",
      "Iteration 32, loss = 0.58301463\n",
      "Iteration 33, loss = 0.57709997\n",
      "Iteration 34, loss = 0.57307158\n",
      "Iteration 35, loss = 0.56633022\n",
      "Iteration 36, loss = 0.56022550\n",
      "Iteration 37, loss = 0.55344735\n",
      "Iteration 38, loss = 0.54957997\n",
      "Iteration 39, loss = 0.54315235\n",
      "Iteration 40, loss = 0.54004605\n",
      "Iteration 41, loss = 0.53356660\n",
      "Iteration 42, loss = 0.52894534\n",
      "Iteration 43, loss = 0.52270887\n",
      "Iteration 44, loss = 0.51746238\n",
      "Iteration 45, loss = 0.51297094\n",
      "Iteration 46, loss = 0.50876553\n",
      "Iteration 47, loss = 0.50365215\n",
      "Iteration 48, loss = 0.49911377\n",
      "Iteration 49, loss = 0.49358903\n",
      "Iteration 50, loss = 0.49024272\n",
      "Iteration 51, loss = 0.48482187\n",
      "Iteration 52, loss = 0.48204679\n",
      "Iteration 53, loss = 0.47783504\n",
      "Iteration 54, loss = 0.47249794\n",
      "Iteration 55, loss = 0.46972766\n",
      "Iteration 56, loss = 0.46460887\n",
      "Iteration 57, loss = 0.46078914\n",
      "Iteration 58, loss = 0.45829266\n",
      "Iteration 59, loss = 0.45343127\n",
      "Iteration 60, loss = 0.44949128\n",
      "Iteration 61, loss = 0.44627689\n",
      "Iteration 62, loss = 0.44314546\n",
      "Iteration 63, loss = 0.43914656\n",
      "Iteration 64, loss = 0.43393486\n",
      "Iteration 65, loss = 0.43067487\n",
      "Iteration 66, loss = 0.42935655\n",
      "Iteration 67, loss = 0.42911594\n",
      "Iteration 68, loss = 0.41998197\n",
      "Iteration 69, loss = 0.41728959\n",
      "Iteration 70, loss = 0.41475482\n",
      "Iteration 71, loss = 0.41154101\n",
      "Iteration 72, loss = 0.40749068\n",
      "Iteration 73, loss = 0.40721636\n",
      "Iteration 74, loss = 0.40127556\n",
      "Iteration 75, loss = 0.39919329\n",
      "Iteration 76, loss = 0.39664438\n",
      "Iteration 77, loss = 0.39153298\n",
      "Iteration 78, loss = 0.39035141\n",
      "Iteration 79, loss = 0.38840904\n",
      "Iteration 80, loss = 0.38422469\n",
      "Iteration 81, loss = 0.38145389\n",
      "Iteration 82, loss = 0.38136834\n",
      "Iteration 83, loss = 0.37868118\n",
      "Iteration 84, loss = 0.37484079\n",
      "Iteration 85, loss = 0.37126854\n",
      "Iteration 86, loss = 0.36899268\n",
      "Iteration 87, loss = 0.36734814\n",
      "Iteration 88, loss = 0.36521879\n",
      "Iteration 89, loss = 0.36280459\n",
      "Iteration 90, loss = 0.36055828\n",
      "Iteration 91, loss = 0.35697480\n",
      "Iteration 92, loss = 0.35357357\n",
      "Iteration 93, loss = 0.35256170\n",
      "Iteration 94, loss = 0.35004974\n",
      "Iteration 95, loss = 0.34838707\n",
      "Iteration 96, loss = 0.34556986\n",
      "Iteration 97, loss = 0.34447008\n",
      "Iteration 98, loss = 0.34132662\n",
      "Iteration 99, loss = 0.33842136\n",
      "Iteration 100, loss = 0.33707237\n",
      "Iteration 101, loss = 0.33387202\n",
      "Iteration 102, loss = 0.33305872\n",
      "Iteration 103, loss = 0.32941022\n",
      "Iteration 104, loss = 0.32957027\n",
      "Iteration 105, loss = 0.32680915\n",
      "Iteration 106, loss = 0.32427527\n",
      "Iteration 107, loss = 0.32193245\n",
      "Iteration 108, loss = 0.32452811\n",
      "Iteration 109, loss = 0.32155880\n",
      "Iteration 110, loss = 0.32180143\n",
      "Iteration 111, loss = 0.31678232\n",
      "Iteration 112, loss = 0.31323587\n",
      "Iteration 113, loss = 0.31180141\n",
      "Iteration 114, loss = 0.30937125\n",
      "Iteration 115, loss = 0.30732865\n",
      "Iteration 116, loss = 0.30676394\n",
      "Iteration 117, loss = 0.30630403\n",
      "Iteration 118, loss = 0.30437217\n",
      "Iteration 119, loss = 0.30262847\n",
      "Iteration 120, loss = 0.30029085\n",
      "Iteration 121, loss = 0.29859769\n",
      "Iteration 122, loss = 0.29624483\n",
      "Iteration 123, loss = 0.29426305\n",
      "Iteration 124, loss = 0.29321571\n",
      "Iteration 125, loss = 0.29234309\n",
      "Iteration 126, loss = 0.29043783\n",
      "Iteration 127, loss = 0.28900752\n",
      "Iteration 128, loss = 0.28617761\n",
      "Iteration 129, loss = 0.28507519\n",
      "Iteration 130, loss = 0.28260895\n",
      "Iteration 131, loss = 0.28069037\n",
      "Iteration 132, loss = 0.28041148\n",
      "Iteration 133, loss = 0.27731997\n",
      "Iteration 134, loss = 0.27832824\n",
      "Iteration 135, loss = 0.27500665\n",
      "Iteration 136, loss = 0.27421589\n",
      "Iteration 137, loss = 0.27198199\n",
      "Iteration 138, loss = 0.27231484\n",
      "Iteration 139, loss = 0.27014450\n",
      "Iteration 140, loss = 0.26969123\n",
      "Iteration 141, loss = 0.26551803\n",
      "Iteration 142, loss = 0.26487655\n",
      "Iteration 143, loss = 0.26417441\n",
      "Iteration 144, loss = 0.26173640\n",
      "Iteration 145, loss = 0.25926974\n",
      "Iteration 146, loss = 0.25878364\n",
      "Iteration 147, loss = 0.25774236\n",
      "Iteration 148, loss = 0.25793698\n",
      "Iteration 149, loss = 0.25725929\n",
      "Iteration 150, loss = 0.25359644\n",
      "Iteration 151, loss = 0.25263229\n",
      "Iteration 152, loss = 0.25355138\n",
      "Iteration 153, loss = 0.25139777\n",
      "Iteration 154, loss = 0.24887896\n",
      "Iteration 155, loss = 0.24677487\n",
      "Iteration 156, loss = 0.24549998\n",
      "Iteration 157, loss = 0.24492547\n",
      "Iteration 158, loss = 0.24357665\n",
      "Iteration 159, loss = 0.24120574\n",
      "Iteration 160, loss = 0.24017209\n",
      "Iteration 161, loss = 0.23905515\n",
      "Iteration 162, loss = 0.23755661\n",
      "Iteration 163, loss = 0.23653677\n",
      "Iteration 164, loss = 0.23656018\n",
      "Iteration 165, loss = 0.23389703\n",
      "Iteration 166, loss = 0.23336033\n",
      "Iteration 167, loss = 0.23210987\n",
      "Iteration 168, loss = 0.23116736\n",
      "Iteration 169, loss = 0.22797695\n",
      "Iteration 170, loss = 0.22793085\n",
      "Iteration 171, loss = 0.22622195\n",
      "Iteration 172, loss = 0.22597731\n",
      "Iteration 173, loss = 0.22410124\n",
      "Iteration 174, loss = 0.22286425\n",
      "Iteration 175, loss = 0.22434759\n",
      "Iteration 176, loss = 0.22208129\n",
      "Iteration 177, loss = 0.21961028\n",
      "Iteration 178, loss = 0.21902429\n",
      "Iteration 179, loss = 0.21785980\n",
      "Iteration 180, loss = 0.21794869\n",
      "Iteration 181, loss = 0.21627935\n",
      "Iteration 182, loss = 0.21357857\n",
      "Iteration 183, loss = 0.21514447\n",
      "Iteration 184, loss = 0.21257525\n",
      "Iteration 185, loss = 0.21194888\n",
      "Iteration 186, loss = 0.21047252\n",
      "Iteration 187, loss = 0.20932204\n",
      "Iteration 188, loss = 0.20815320\n",
      "Iteration 189, loss = 0.20728872\n",
      "Iteration 190, loss = 0.20698045\n",
      "Iteration 191, loss = 0.20515626\n",
      "Iteration 192, loss = 0.20347741\n",
      "Iteration 193, loss = 0.20298033\n",
      "Iteration 194, loss = 0.20196624\n",
      "Iteration 195, loss = 0.20216761\n",
      "Iteration 196, loss = 0.20023426\n",
      "Iteration 197, loss = 0.19799370\n",
      "Iteration 198, loss = 0.19821484\n",
      "Iteration 199, loss = 0.19651210\n",
      "Iteration 200, loss = 0.19684117\n",
      "Iteration 201, loss = 0.19507278\n",
      "Iteration 202, loss = 0.19338739\n",
      "Iteration 203, loss = 0.19331348\n",
      "Iteration 204, loss = 0.19238542\n",
      "Iteration 205, loss = 0.19163164\n",
      "Iteration 206, loss = 0.19121880\n",
      "Iteration 207, loss = 0.19008013\n",
      "Iteration 208, loss = 0.18862151\n",
      "Iteration 209, loss = 0.18732162\n",
      "Iteration 210, loss = 0.18703101\n",
      "Iteration 211, loss = 0.18617228\n",
      "Iteration 212, loss = 0.18501703\n",
      "Iteration 213, loss = 0.18576354\n",
      "Iteration 214, loss = 0.18268592\n",
      "Iteration 215, loss = 0.18265568\n",
      "Iteration 216, loss = 0.18093989\n",
      "Iteration 217, loss = 0.18077830\n",
      "Iteration 218, loss = 0.18085591\n",
      "Iteration 219, loss = 0.18031316\n",
      "Iteration 220, loss = 0.17802383\n",
      "Iteration 221, loss = 0.17708416\n",
      "Iteration 222, loss = 0.17615206\n",
      "Iteration 223, loss = 0.17553095\n",
      "Iteration 224, loss = 0.17444396\n",
      "Iteration 225, loss = 0.17391726\n",
      "Iteration 226, loss = 0.17358459\n",
      "Iteration 227, loss = 0.17298031\n",
      "Iteration 228, loss = 0.17309470\n",
      "Iteration 229, loss = 0.17140202\n",
      "Iteration 230, loss = 0.17123154\n",
      "Iteration 231, loss = 0.16948337\n",
      "Iteration 232, loss = 0.16875735\n",
      "Iteration 233, loss = 0.16781789\n",
      "Iteration 234, loss = 0.16844455\n",
      "Iteration 235, loss = 0.16628250\n",
      "Iteration 236, loss = 0.16728657\n",
      "Iteration 237, loss = 0.16506697\n",
      "Iteration 238, loss = 0.16362006\n",
      "Iteration 239, loss = 0.16300247\n",
      "Iteration 240, loss = 0.16363579\n",
      "Iteration 241, loss = 0.16297721\n",
      "Iteration 242, loss = 0.16167461\n",
      "Iteration 243, loss = 0.15949779\n",
      "Iteration 244, loss = 0.15969361\n",
      "Iteration 245, loss = 0.15901129\n",
      "Iteration 246, loss = 0.15990729\n",
      "Iteration 247, loss = 0.15869088\n",
      "Iteration 248, loss = 0.15849853\n",
      "Iteration 249, loss = 0.15459767\n",
      "Iteration 250, loss = 0.15555002\n",
      "Iteration 251, loss = 0.15462639\n",
      "Iteration 252, loss = 0.15394581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.15247875\n",
      "Iteration 254, loss = 0.15146109\n",
      "Iteration 255, loss = 0.15218213\n",
      "Iteration 256, loss = 0.15123744\n",
      "Iteration 257, loss = 0.15013282\n",
      "Iteration 258, loss = 0.14941717\n",
      "Iteration 259, loss = 0.14855212\n",
      "Iteration 260, loss = 0.14698033\n",
      "Iteration 261, loss = 0.14710092\n",
      "Iteration 262, loss = 0.14698538\n",
      "Iteration 263, loss = 0.14568040\n",
      "Iteration 264, loss = 0.14598462\n",
      "Iteration 265, loss = 0.14469317\n",
      "Iteration 266, loss = 0.14447965\n",
      "Iteration 267, loss = 0.14310891\n",
      "Iteration 268, loss = 0.14273884\n",
      "Iteration 269, loss = 0.14184867\n",
      "Iteration 270, loss = 0.14087959\n",
      "Iteration 271, loss = 0.14137226\n",
      "Iteration 272, loss = 0.13988003\n",
      "Iteration 273, loss = 0.14028088\n",
      "Iteration 274, loss = 0.13997962\n",
      "Iteration 275, loss = 0.13778159\n",
      "Iteration 276, loss = 0.13770163\n",
      "Iteration 277, loss = 0.13728425\n",
      "Iteration 278, loss = 0.13702538\n",
      "Iteration 279, loss = 0.13586445\n",
      "Iteration 280, loss = 0.13503811\n",
      "Iteration 281, loss = 0.13420210\n",
      "Iteration 282, loss = 0.13381851\n",
      "Iteration 283, loss = 0.13476526\n",
      "Iteration 284, loss = 0.13389533\n",
      "Iteration 285, loss = 0.13192231\n",
      "Iteration 286, loss = 0.13164958\n",
      "Iteration 287, loss = 0.13086309\n",
      "Iteration 288, loss = 0.13300446\n",
      "Iteration 289, loss = 0.13034755\n",
      "Iteration 290, loss = 0.12908199\n",
      "Iteration 291, loss = 0.12883038\n",
      "Iteration 292, loss = 0.12850943\n",
      "Iteration 293, loss = 0.12885619\n",
      "Iteration 294, loss = 0.12588392\n",
      "Iteration 295, loss = 0.12755908\n",
      "Iteration 296, loss = 0.12693653\n",
      "Iteration 297, loss = 0.12571736\n",
      "Iteration 298, loss = 0.12594637\n",
      "Iteration 299, loss = 0.12524387\n",
      "Iteration 300, loss = 0.12524476\n",
      "Iteration 301, loss = 0.12352178\n",
      "Iteration 302, loss = 0.12250954\n",
      "Iteration 303, loss = 0.12153200\n",
      "Iteration 304, loss = 0.12262363\n",
      "Iteration 305, loss = 0.12366566\n",
      "Iteration 306, loss = 0.12102806\n",
      "Iteration 307, loss = 0.12118728\n",
      "Iteration 308, loss = 0.12208454\n",
      "Iteration 309, loss = 0.11873704\n",
      "Iteration 310, loss = 0.11876936\n",
      "Iteration 311, loss = 0.11913167\n",
      "Iteration 312, loss = 0.12138774\n",
      "Iteration 313, loss = 0.11688944\n",
      "Iteration 314, loss = 0.11793922\n",
      "Iteration 315, loss = 0.11566954\n",
      "Iteration 316, loss = 0.11535884\n",
      "Iteration 317, loss = 0.11484230\n",
      "Iteration 318, loss = 0.11677889\n",
      "Iteration 319, loss = 0.11366538\n",
      "Iteration 320, loss = 0.11298932\n",
      "Iteration 321, loss = 0.11385695\n",
      "Iteration 322, loss = 0.11307592\n",
      "Iteration 323, loss = 0.11264911\n",
      "Iteration 324, loss = 0.11184961\n",
      "Iteration 325, loss = 0.11336557\n",
      "Iteration 326, loss = 0.11066085\n",
      "Iteration 327, loss = 0.11045972\n",
      "Iteration 328, loss = 0.10953461\n",
      "Iteration 329, loss = 0.10900103\n",
      "Iteration 330, loss = 0.10939208\n",
      "Iteration 331, loss = 0.11029567\n",
      "Iteration 332, loss = 0.10846387\n",
      "Iteration 333, loss = 0.11076836\n",
      "Iteration 334, loss = 0.10759017\n",
      "Iteration 335, loss = 0.10763591\n",
      "Iteration 336, loss = 0.10649158\n",
      "Iteration 337, loss = 0.10680941\n",
      "Iteration 338, loss = 0.10587185\n",
      "Iteration 339, loss = 0.10411863\n",
      "Iteration 340, loss = 0.10422070\n",
      "Iteration 341, loss = 0.10450387\n",
      "Iteration 342, loss = 0.10393961\n",
      "Iteration 343, loss = 0.10290474\n",
      "Iteration 344, loss = 0.10284780\n",
      "Iteration 345, loss = 0.10266849\n",
      "Iteration 346, loss = 0.10183711\n",
      "Iteration 347, loss = 0.10071884\n",
      "Iteration 348, loss = 0.10240109\n",
      "Iteration 349, loss = 0.10184233\n",
      "Iteration 350, loss = 0.10281442\n",
      "Iteration 351, loss = 0.10163489\n",
      "Iteration 352, loss = 0.09972717\n",
      "Iteration 353, loss = 0.09907342\n",
      "Iteration 354, loss = 0.09898644\n",
      "Iteration 355, loss = 0.09823678\n",
      "Iteration 356, loss = 0.09881403\n",
      "Iteration 357, loss = 0.09848474\n",
      "Iteration 358, loss = 0.09752123\n",
      "Iteration 359, loss = 0.09751547\n",
      "Iteration 360, loss = 0.09646970\n",
      "Iteration 361, loss = 0.09526296\n",
      "Iteration 362, loss = 0.09505484\n",
      "Iteration 363, loss = 0.09539093\n",
      "Iteration 364, loss = 0.09519431\n",
      "Iteration 365, loss = 0.09586994\n",
      "Iteration 366, loss = 0.09329834\n",
      "Iteration 367, loss = 0.09461204\n",
      "Iteration 368, loss = 0.09308981\n",
      "Iteration 369, loss = 0.09235117\n",
      "Iteration 370, loss = 0.09405500\n",
      "Iteration 371, loss = 0.09259695\n",
      "Iteration 372, loss = 0.09130335\n",
      "Iteration 373, loss = 0.09305411\n",
      "Iteration 374, loss = 0.09238920\n",
      "Iteration 375, loss = 0.09084477\n",
      "Iteration 376, loss = 0.09032870\n",
      "Iteration 377, loss = 0.09005037\n",
      "Iteration 378, loss = 0.08900520\n",
      "Iteration 379, loss = 0.08896585\n",
      "Iteration 380, loss = 0.08954743\n",
      "Iteration 381, loss = 0.08936370\n",
      "Iteration 382, loss = 0.08887070\n",
      "Iteration 383, loss = 0.08691775\n",
      "Iteration 384, loss = 0.08791836\n",
      "Iteration 385, loss = 0.08708872\n",
      "Iteration 386, loss = 0.08592395\n",
      "Iteration 387, loss = 0.08709235\n",
      "Iteration 388, loss = 0.08663739\n",
      "Iteration 389, loss = 0.08561753\n",
      "Iteration 390, loss = 0.08562958\n",
      "Iteration 391, loss = 0.08514374\n",
      "Iteration 392, loss = 0.08520815\n",
      "Iteration 393, loss = 0.08582022\n",
      "Iteration 394, loss = 0.08460250\n",
      "Iteration 395, loss = 0.08422786\n",
      "Iteration 396, loss = 0.08217152\n",
      "Iteration 397, loss = 0.08246290\n",
      "Iteration 398, loss = 0.08243009\n",
      "Iteration 399, loss = 0.08250051\n",
      "Iteration 400, loss = 0.08213776\n",
      "Iteration 401, loss = 0.08309316\n",
      "Iteration 402, loss = 0.08214209\n",
      "Iteration 403, loss = 0.08352227\n",
      "Iteration 404, loss = 0.08003927\n",
      "Iteration 405, loss = 0.08374291\n",
      "Iteration 406, loss = 0.08017901\n",
      "Iteration 407, loss = 0.08068794\n",
      "Iteration 408, loss = 0.07975880\n",
      "Iteration 409, loss = 0.07978117\n",
      "Iteration 410, loss = 0.07905294\n",
      "Iteration 411, loss = 0.07815069\n",
      "Iteration 412, loss = 0.07796861\n",
      "Iteration 413, loss = 0.07854348\n",
      "Iteration 414, loss = 0.07937982\n",
      "Iteration 415, loss = 0.07804587\n",
      "Iteration 416, loss = 0.07699688\n",
      "Iteration 417, loss = 0.07711340\n",
      "Iteration 418, loss = 0.07692830\n",
      "Iteration 419, loss = 0.07738648\n",
      "Iteration 420, loss = 0.07596555\n",
      "Iteration 421, loss = 0.07577901\n",
      "Iteration 422, loss = 0.07521970\n",
      "Iteration 423, loss = 0.07448082\n",
      "Iteration 424, loss = 0.07365258\n",
      "Iteration 425, loss = 0.07558647\n",
      "Iteration 426, loss = 0.07375637\n",
      "Iteration 427, loss = 0.07568587\n",
      "Iteration 428, loss = 0.07586247\n",
      "Iteration 429, loss = 0.07307616\n",
      "Iteration 430, loss = 0.07329128\n",
      "Iteration 431, loss = 0.07510102\n",
      "Iteration 432, loss = 0.07288979\n",
      "Iteration 433, loss = 0.07324582\n",
      "Iteration 434, loss = 0.07251945\n",
      "Iteration 435, loss = 0.07280759\n",
      "Iteration 436, loss = 0.07282191\n",
      "Iteration 437, loss = 0.07235808\n",
      "Iteration 438, loss = 0.07126666\n",
      "Iteration 439, loss = 0.07098761\n",
      "Iteration 440, loss = 0.07026561\n",
      "Iteration 441, loss = 0.06989849\n",
      "Iteration 442, loss = 0.07062642\n",
      "Iteration 443, loss = 0.06986970\n",
      "Iteration 444, loss = 0.06940022\n",
      "Iteration 445, loss = 0.07030496\n",
      "Iteration 446, loss = 0.06769052\n",
      "Iteration 447, loss = 0.06892797\n",
      "Iteration 448, loss = 0.06955208\n",
      "Iteration 449, loss = 0.07001455\n",
      "Iteration 450, loss = 0.06908986\n",
      "Iteration 451, loss = 0.06854319\n",
      "Iteration 452, loss = 0.06817305\n",
      "Iteration 453, loss = 0.06727798\n",
      "Iteration 454, loss = 0.06694262\n",
      "Iteration 455, loss = 0.06794956\n",
      "Iteration 456, loss = 0.06639103\n",
      "Iteration 457, loss = 0.06686245\n",
      "Iteration 458, loss = 0.06698685\n",
      "Iteration 459, loss = 0.06648224\n",
      "Iteration 460, loss = 0.06704808\n",
      "Iteration 461, loss = 0.06701483\n",
      "Iteration 462, loss = 0.06647783\n",
      "Iteration 463, loss = 0.06589265\n",
      "Iteration 464, loss = 0.06526036\n",
      "Iteration 465, loss = 0.06499116\n",
      "Iteration 466, loss = 0.06465919\n",
      "Iteration 467, loss = 0.06476685\n",
      "Iteration 468, loss = 0.06373004\n",
      "Iteration 469, loss = 0.06432518\n",
      "Iteration 470, loss = 0.06476189\n",
      "Iteration 471, loss = 0.06350297\n",
      "Iteration 472, loss = 0.06284070\n",
      "Iteration 473, loss = 0.06340324\n",
      "Iteration 474, loss = 0.06278531\n",
      "Iteration 475, loss = 0.06357618\n",
      "Iteration 476, loss = 0.06349258\n",
      "Iteration 477, loss = 0.06262563\n",
      "Iteration 478, loss = 0.06228541\n",
      "Iteration 479, loss = 0.06110872\n",
      "Iteration 480, loss = 0.06178328\n",
      "Iteration 481, loss = 0.06222970\n",
      "Iteration 482, loss = 0.06182679\n",
      "Iteration 483, loss = 0.06139855\n",
      "Iteration 484, loss = 0.06064713\n",
      "Iteration 485, loss = 0.06371116\n",
      "Iteration 486, loss = 0.06183279\n",
      "Iteration 487, loss = 0.05956110\n",
      "Iteration 488, loss = 0.06001961\n",
      "Iteration 489, loss = 0.05873015\n",
      "Iteration 490, loss = 0.05912165\n",
      "Iteration 491, loss = 0.06121476\n",
      "Iteration 492, loss = 0.05921526\n",
      "Iteration 493, loss = 0.05940748\n",
      "Iteration 494, loss = 0.05943842\n",
      "Iteration 495, loss = 0.05886494\n",
      "Iteration 496, loss = 0.06065608\n",
      "Iteration 497, loss = 0.05896383\n",
      "Iteration 498, loss = 0.05903855\n",
      "Iteration 499, loss = 0.05803269\n",
      "Iteration 500, loss = 0.05822568\n",
      "Iteration 501, loss = 0.05811868\n",
      "Iteration 502, loss = 0.05779094\n",
      "Iteration 503, loss = 0.05776608\n",
      "Iteration 504, loss = 0.05649587\n",
      "Iteration 505, loss = 0.05795420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 506, loss = 0.05873660\n",
      "Iteration 507, loss = 0.05635059\n",
      "Iteration 508, loss = 0.05820322\n",
      "Iteration 509, loss = 0.05582131\n",
      "Iteration 510, loss = 0.05576694\n",
      "Iteration 511, loss = 0.05471651\n",
      "Iteration 512, loss = 0.05494052\n",
      "Iteration 513, loss = 0.05493207\n",
      "Iteration 514, loss = 0.05684593\n",
      "Iteration 515, loss = 0.05585284\n",
      "Iteration 516, loss = 0.05471078\n",
      "Iteration 517, loss = 0.05484153\n",
      "Iteration 518, loss = 0.05735221\n",
      "Iteration 519, loss = 0.05406821\n",
      "Iteration 520, loss = 0.05469494\n",
      "Iteration 521, loss = 0.05549739\n",
      "Iteration 522, loss = 0.05295252\n",
      "Iteration 523, loss = 0.05508189\n",
      "Iteration 524, loss = 0.05490073\n",
      "Iteration 525, loss = 0.05323956\n",
      "Iteration 526, loss = 0.05305089\n",
      "Iteration 527, loss = 0.05491668\n",
      "Iteration 528, loss = 0.05412531\n",
      "Iteration 529, loss = 0.05298907\n",
      "Iteration 530, loss = 0.05422888\n",
      "Iteration 531, loss = 0.05233433\n",
      "Iteration 532, loss = 0.05192303\n",
      "Iteration 533, loss = 0.05092770\n",
      "Iteration 534, loss = 0.05265787\n",
      "Iteration 535, loss = 0.05166949\n",
      "Iteration 536, loss = 0.05271959\n",
      "Iteration 537, loss = 0.05173802\n",
      "Iteration 538, loss = 0.05127949\n",
      "Iteration 539, loss = 0.05190119\n",
      "Iteration 540, loss = 0.05074609\n",
      "Iteration 541, loss = 0.05136521\n",
      "Iteration 542, loss = 0.05112209\n",
      "Iteration 543, loss = 0.05023455\n",
      "Iteration 544, loss = 0.05067250\n",
      "Iteration 545, loss = 0.04909335\n",
      "Iteration 546, loss = 0.04977679\n",
      "Iteration 547, loss = 0.05085816\n",
      "Iteration 548, loss = 0.04953942\n",
      "Iteration 549, loss = 0.05035558\n",
      "Iteration 550, loss = 0.04892605\n",
      "Iteration 551, loss = 0.04982767\n",
      "Iteration 552, loss = 0.04957770\n",
      "Iteration 553, loss = 0.04860137\n",
      "Iteration 554, loss = 0.04823542\n",
      "Iteration 555, loss = 0.04811493\n",
      "Iteration 556, loss = 0.04842990\n",
      "Iteration 557, loss = 0.04908980\n",
      "Iteration 558, loss = 0.04943911\n",
      "Iteration 559, loss = 0.04788597\n",
      "Iteration 560, loss = 0.04763846\n",
      "Iteration 561, loss = 0.04708559\n",
      "Iteration 562, loss = 0.04703976\n",
      "Iteration 563, loss = 0.04755499\n",
      "Iteration 564, loss = 0.05028378\n",
      "Iteration 565, loss = 0.04948407\n",
      "Iteration 566, loss = 0.04775944\n",
      "Iteration 567, loss = 0.04809305\n",
      "Iteration 568, loss = 0.05067323\n",
      "Iteration 569, loss = 0.04767749\n",
      "Iteration 570, loss = 0.04770499\n",
      "Iteration 571, loss = 0.04708559\n",
      "Iteration 572, loss = 0.04624146\n",
      "Iteration 573, loss = 0.04483321\n",
      "Iteration 574, loss = 0.04616659\n",
      "Iteration 575, loss = 0.04590368\n",
      "Iteration 576, loss = 0.04678783\n",
      "Iteration 577, loss = 0.04530667\n",
      "Iteration 578, loss = 0.04582521\n",
      "Iteration 579, loss = 0.04736304\n",
      "Iteration 580, loss = 0.04700585\n",
      "Iteration 581, loss = 0.04642928\n",
      "Iteration 582, loss = 0.04541557\n",
      "Iteration 583, loss = 0.04460941\n",
      "Iteration 584, loss = 0.04486509\n",
      "Iteration 585, loss = 0.04546089\n",
      "Iteration 586, loss = 0.04376069\n",
      "Iteration 587, loss = 0.04690428\n",
      "Iteration 588, loss = 0.04457646\n",
      "Iteration 589, loss = 0.04496857\n",
      "Iteration 590, loss = 0.04458957\n",
      "Iteration 591, loss = 0.04354379\n",
      "Iteration 592, loss = 0.04375770\n",
      "Iteration 593, loss = 0.04549210\n",
      "Iteration 594, loss = 0.04363376\n",
      "Iteration 595, loss = 0.04373206\n",
      "Iteration 596, loss = 0.04331709\n",
      "Iteration 597, loss = 0.04311589\n",
      "Iteration 598, loss = 0.04317111\n",
      "Iteration 599, loss = 0.04338689\n",
      "Iteration 600, loss = 0.04546024\n",
      "Iteration 601, loss = 0.04327276\n",
      "Iteration 602, loss = 0.04202433\n",
      "Iteration 603, loss = 0.04273749\n",
      "Iteration 604, loss = 0.04278026\n",
      "Iteration 605, loss = 0.04351664\n",
      "Iteration 606, loss = 0.04344536\n",
      "Iteration 607, loss = 0.04296838\n",
      "Iteration 608, loss = 0.04249545\n",
      "Iteration 609, loss = 0.04250882\n",
      "Iteration 610, loss = 0.04261192\n",
      "Iteration 611, loss = 0.04250864\n",
      "Iteration 612, loss = 0.04213629\n",
      "Iteration 613, loss = 0.04140896\n",
      "Iteration 614, loss = 0.04133452\n",
      "Iteration 615, loss = 0.04114042\n",
      "Iteration 616, loss = 0.04002204\n",
      "Iteration 617, loss = 0.04224965\n",
      "Iteration 618, loss = 0.04134045\n",
      "Iteration 619, loss = 0.04067899\n",
      "Iteration 620, loss = 0.04026754\n",
      "Iteration 621, loss = 0.04008447\n",
      "Iteration 622, loss = 0.03991800\n",
      "Iteration 623, loss = 0.04064440\n",
      "Iteration 624, loss = 0.03982401\n",
      "Iteration 625, loss = 0.04123383\n",
      "Iteration 626, loss = 0.03987376\n",
      "Iteration 627, loss = 0.03938125\n",
      "Iteration 628, loss = 0.04025169\n",
      "Iteration 629, loss = 0.04035920\n",
      "Iteration 630, loss = 0.03943494\n",
      "Iteration 631, loss = 0.03900727\n",
      "Iteration 632, loss = 0.03975516\n",
      "Iteration 633, loss = 0.04061738\n",
      "Iteration 634, loss = 0.03822637\n",
      "Iteration 635, loss = 0.03927349\n",
      "Iteration 636, loss = 0.04010533\n",
      "Iteration 637, loss = 0.03906677\n",
      "Iteration 638, loss = 0.03851169\n",
      "Iteration 639, loss = 0.03897338\n",
      "Iteration 640, loss = 0.03877176\n",
      "Iteration 641, loss = 0.03884372\n",
      "Iteration 642, loss = 0.03885174\n",
      "Iteration 643, loss = 0.03782585\n",
      "Iteration 644, loss = 0.03807088\n",
      "Iteration 645, loss = 0.03912040\n",
      "Iteration 646, loss = 0.03911193\n",
      "Iteration 647, loss = 0.03955756\n",
      "Iteration 648, loss = 0.04026583\n",
      "Iteration 649, loss = 0.03886390\n",
      "Iteration 650, loss = 0.03800215\n",
      "Iteration 651, loss = 0.03702261\n",
      "Iteration 652, loss = 0.03922564\n",
      "Iteration 653, loss = 0.03856853\n",
      "Iteration 654, loss = 0.03815286\n",
      "Iteration 655, loss = 0.03559689\n",
      "Iteration 656, loss = 0.03667379\n",
      "Iteration 657, loss = 0.03895546\n",
      "Iteration 658, loss = 0.03633590\n",
      "Iteration 659, loss = 0.03885307\n",
      "Iteration 660, loss = 0.03731069\n",
      "Iteration 661, loss = 0.03808982\n",
      "Iteration 662, loss = 0.03705896\n",
      "Iteration 663, loss = 0.03678369\n",
      "Iteration 664, loss = 0.03649617\n",
      "Iteration 665, loss = 0.03679267\n",
      "Iteration 666, loss = 0.03761787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Default settings create 1 hidden layer with 100 neurons\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, verbose=True)\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951321279554938"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7920181700194678"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase neurons from 100 to 50\n",
    "mlp_clf = MLPClassifier(max_iter=1000, verbose=False,\n",
    "                        hidden_layer_sizes=(50,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9680111265646731"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472420506164829"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers:\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791376912378303"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7410772225827384"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 80, 60, 40, 20, 10), max_iter=1000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(100,80,60,40,20,10),\n",
    "                       max_iter=1000) # Funnel Shape 100,80,60,40,20,10 # Pipe Shape 30, 30, 30\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers:\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9796940194714882"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7871512005191434"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=(50, 25, 10),\n",
       "              max_iter=1000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960500695410292"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7138221933809215"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 25, 10), max_iter=1000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'tanh') #\"logistic\"\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958275382475661"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803374432186891"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver (Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erich\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 25, 10), max_iter=1000,\n",
       "              solver='sgd')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's use Stochastic Gradient Descent optimizer\n",
    "\n",
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'tanh',\n",
    "                       solver='sgd')\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210013908205842"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810512654120701"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44671982,  0.71670022,  0.61324911,  1.89706592,  1.08766162,\n",
       "         0.4710924 ,  0.02385204, -0.40589026, -0.57249234, -0.00472628,\n",
       "        -0.33824687,  1.57366607,  0.51641975,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select a random observation\n",
    "\n",
    "random = test_x[50:51]\n",
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.75421203e-02, 3.87056487e-05, 9.82418299e-01, 8.75408730e-07]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the input variables of the observation\n",
    "dnn_clf.predict_proba(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02, 0.  , 0.98, 0.  ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round the probability values\n",
    "np.round(dnn_clf.predict_proba(random), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
